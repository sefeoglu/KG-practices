{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Baselines Models.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMuEgJ7i5O7zM3vBLoIKxEl"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPKRtwGmS2To"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCTCwZ9WA9o7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f928969-5839-4aaf-dc63-88872470e98a"
      },
      "source": [
        "pip install openbiolink==0.1.4"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openbiolink==0.1.4 in /usr/local/lib/python3.7/dist-packages (0.1.4)\n",
            "Requirement already satisfied: pandas>=0.23.4 in /usr/local/lib/python3.7/dist-packages (from openbiolink==0.1.4) (1.1.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from openbiolink==0.1.4) (7.1.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from openbiolink==0.1.4) (1.9.0+cu102)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from openbiolink==0.1.4) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.29.1 in /usr/local/lib/python3.7/dist-packages (from openbiolink==0.1.4) (4.62.3)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.4->openbiolink==0.1.4) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.4->openbiolink==0.1.4) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.23.4->openbiolink==0.1.4) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->openbiolink==0.1.4) (3.7.4.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeOnH20Vy-YJ"
      },
      "source": [
        "# ComplexIE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8E5_vu9VzAbY",
        "outputId": "4561e33e-5cad-47bf-c3f0-7578ddb496c2"
      },
      "source": [
        "!git clone https://github.com/ttrouill/complex.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'complex'...\n",
            "remote: Enumerating objects: 93, done.\u001b[K\n",
            "remote: Total 93 (delta 0), reused 0 (delta 0), pack-reused 93\u001b[K\n",
            "Unpacking objects: 100% (93/93), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUNz67AKzGYQ",
        "outputId": "484242df-338d-400f-ead5-5a51439ecf5e"
      },
      "source": [
        "pip install -r KnowledgeGraphEmbedding/requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from -r KnowledgeGraphEmbedding/requirements.txt (line 2)) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn>=0.20.2 in /usr/local/lib/python3.7/dist-packages (from -r KnowledgeGraphEmbedding/requirements.txt (line 3)) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.2->-r KnowledgeGraphEmbedding/requirements.txt (line 3)) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.2->-r KnowledgeGraphEmbedding/requirements.txt (line 3)) (1.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FkDxG9NYVDx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSGLdffIzwa7"
      },
      "source": [
        "!unzip complex/datasets/fb15k.zip -d complex/datasets/\n",
        "!unzip complex/datasets/wn18.zip -d complex/datasets/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foATuS_7z5ec",
        "outputId": "0632f175-a50e-417e-cc6e-8f6b501a9a3e"
      },
      "source": [
        "!python complex/fb15k_run.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "python3: can't open file 'complex/fb15k_run.py': [Errno 2] No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOE-BA7gzSmz",
        "outputId": "eff81b75-6d69-4344-c510-cbabb91d7807"
      },
      "source": [
        "import complex.efe\n",
        "from complex.efe.exp_generators import *\n",
        "import complex.efe.tools as tools\n",
        "\n",
        "if __name__ ==\"__main__\":\n",
        "\n",
        "\t#Load data, ensure that data is at path: 'path'/'name'/[train|valid|test].txt\n",
        "\twn18exp = build_data(name = 'wn18',path =  'complex/datasets/')\n",
        "\n",
        "\n",
        "\t#SGD hyper-parameters:\n",
        "\tparams = Parameters(learning_rate = 0.5, \n",
        "\t\t\t\t\t\tmax_iter = 1000, \n",
        "\t\t\t\t\t\tbatch_size = int(len(wn18exp.train.values) / 100),  #Make 100 batches\n",
        "\t\t\t\t\t\tneg_ratio = 1, \n",
        "\t\t\t\t\t\tvalid_scores_every = 50,\n",
        "\t\t\t\t\t\tlearning_rate_policy = 'adagrad',\n",
        "\t\t\t\t\t\tcontiguous_sampling = False )\n",
        "\n",
        "\t#Here each model is identified by its name, i.e. the string of its class name in models.py\n",
        "\t#Parameters given here are the best ones for each model, validated from the grid-search described in the paper\n",
        "\tall_params = { \"Complex_Logistic_Model\" : params } ; emb_size = 150; lmbda =0.03;\n",
        "\t#all_params = { \"DistMult_Logistic_Model\" : params } ; emb_size = 200; lmbda =0.003; params.learning_rate = 1.0\n",
        "\t#all_params = { \"CP_Logistic_Model\" : params } ; emb_size = 100; lmbda =0.1; \n",
        "\t#all_params = { \"Rescal_Logistic_Model\" : params } ; emb_size = 50; lmbda =0.1\n",
        "\t#all_params = { \"TransE_L2_Model\" : params } ; emb_size = 200; lmbda = 0.5 ; params.learning_rate=0.01"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2021-10-05 20:44:25,398\t(EFE)\t[INFO]\tNb entities: 40943\n",
            "2021-10-05 20:44:25,401\t(EFE)\t[INFO]\tNb relations: 18\n",
            "2021-10-05 20:44:25,406\t(EFE)\t[INFO]\tNb obs triples: 141442\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPbVl7FM4XSR"
      },
      "source": [
        "from complex.efe.models import Complex_Model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9kLOvlxAIjL"
      },
      "source": [
        "import complex.efe\n",
        "from complex.efe.exp_generators import *\n",
        "import complex.efe.tools as tools\n",
        "\n",
        "if __name__ ==\"__main__\":\n",
        "\n",
        "\t#Load data, ensure that data is at path: 'path'/'name'/[train|valid|test].txt\n",
        "\twn18exp = build_data(name = 'wn18',path = 'complex/datasets')\n",
        "\n",
        "\n",
        "\t#SGD hyper-parameters:\n",
        "\tparams = Parameters(learning_rate = 0.5, \n",
        "\t\t\t\t\t\tmax_iter = 1000, \n",
        "\t\t\t\t\t\tbatch_size = int(len(wn18exp.train.values) / 100),  #Make 100 batches\n",
        "\t\t\t\t\t\tneg_ratio = 1, \n",
        "\t\t\t\t\t\tvalid_scores_every = 50,\n",
        "\t\t\t\t\t\tlearning_rate_policy = 'adagrad',\n",
        "\t\t\t\t\t\tcontiguous_sampling = False )\n",
        "\n",
        "\t#Here each model is identified by its name, i.e. the string of its class name in models.py\n",
        "\t#Parameters given here are the best ones for each model, validated from the grid-search described in the paper\n",
        "\tall_params = { \"Complex_Logistic_Model\" : params } ; emb_size = 150; lmbda =0.03;\n",
        "\t#all_p\n",
        "\trams = { \"DistMult_Logistic_Model\" : params } ; emb_size = 200; lmbda =0.003; params.learning_rate = 1.0\n",
        "\t#all_params = { \"CP_Logistic_Model\" : params } ; emb_size = 100; lmbda =0.1; \n",
        "\t#all_params = { \"Rescal_Logistic_Model\" : params } ; emb_size = 50; lmbda =0.1\n",
        "\t#all_params = { \"TransE_L2_Model\" : params } ; emb_size = 200; lmbda = 0.5 ; params.learning_rate=0.01\n",
        "\n",
        "\n",
        "\n",
        "\ttools.logger.info( \"Learning rate: \" + str(params.learning_rate))\n",
        "\ttools.logger.info( \"Max iter: \" + str(params.max_iter))\n",
        "\ttools.logger.info( \"Generated negatives ratio: \" + str(params.neg_ratio))\n",
        "\ttools.logger.info( \"Batch size: \" + str(params.batch_size))\n",
        "\n",
        "\n",
        "\t#Then call a local grid search, here only with one value of rank and regularization\n",
        "\twn18exp.grid_search_on_all_models(all_params, embedding_size_grid = [emb_size], lmbda_grid = [lmbda], nb_runs = 1)\n",
        "\n",
        "\t#Print best averaged metrics:\n",
        "\twn18exp.print_best_MRR_and_hits()\n",
        "\n",
        "\t#Print best averaged metrics per relation:\n",
        "\twn18exp.print_best_MRR_and_hits_per_rel()\n",
        "\n",
        "\n",
        "\n",
        "\t#Save ComplEx embeddings (last trained model, not best on grid search if multiple embedding sizes and lambdas)\n",
        "\t#e1 = wn18exp.models[\"Complex_Logistic_Model\"][0].e1.get_value(borrow=True)\n",
        "\t#e2 = wn18exp.models[\"Complex_Logistic_Model\"][0].e2.get_value(borrow=True)\n",
        "\t#r1 = wn18exp.models[\"Complex_Logistic_Model\"][0].r1.get_value(borrow=True)\n",
        "\t#r2 = wn18exp.models[\"Complex_Logistic_Model\"][0].r2.get_value(borrow=True)\n",
        "\t#scipy.io.savemat('complex_embeddings.mat', \\\n",
        "\t#\t\t{'entities_real' : e1, 'relations_real' : r1, 'entities_imag' : e2, 'relations_imag' : r2  })"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XT55iWLUIV6"
      },
      "source": [
        "from keras import backend as K\n",
        "from os import environ\n",
        "from importlib import reload\n",
        "# user defined function to change keras backend\n",
        "def set_keras_backend(backend):\n",
        "    if K.backend() != backend:\n",
        "       environ['KERAS_BACKEND'] = backend\n",
        "       reload(K)\n",
        "\n",
        "# call the function with \"theano\"\n",
        "set_keras_backend(\"theano\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gbTH9KqX7jq"
      },
      "source": [
        "# ROTE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoSA2K8zSk5a"
      },
      "source": [
        "## Baseline Codes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDfDgbpkX9lF",
        "outputId": "25723bf3-6516-45b4-ad95-303e08ec1809"
      },
      "source": [
        "!git clone https://github.com/DeepGraphLearning/KnowledgeGraphEmbedding.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'KnowledgeGraphEmbedding'...\n",
            "remote: Enumerating objects: 113, done.\u001b[K\n",
            "remote: Total 113 (delta 0), reused 0 (delta 0), pack-reused 113\u001b[K\n",
            "Receiving objects: 100% (113/113), 32.19 MiB | 15.73 MiB/s, done.\n",
            "Resolving deltas: 100% (35/35), done.\n",
            "Checking out files: 100% (55/55), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZFIwhCwYBD3",
        "outputId": "8ec9f0b3-d4d0-41fc-8812-73686ef67db7"
      },
      "source": [
        "pip install -r KnowledgeGraphEmbedding/requirements.txt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from -r KnowledgeGraphEmbedding/requirements.txt (line 2)) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn>=0.20.2 in /usr/local/lib/python3.7/dist-packages (from -r KnowledgeGraphEmbedding/requirements.txt (line 3)) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.2->-r KnowledgeGraphEmbedding/requirements.txt (line 3)) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.2->-r KnowledgeGraphEmbedding/requirements.txt (line 3)) (1.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-0ieZSnZJ0G",
        "outputId": "6fced7e1-74ed-4c01-b9df-705bfdd16c08"
      },
      "source": [
        "!bash KnowledgeGraphEmbedding/run.sh train RotatE FB15k 0 0 1024 256 1000 24.0 1.0 0.0001 2000 16 -de"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.9.0+cu102\n",
            "Start Training......\n",
            "2021-10-06 11:56:53,587 INFO     Model: RotatE\n",
            "2021-10-06 11:56:53,588 INFO     Data Path: KnowledgeGraphEmbedding/data/FB15k\n",
            "2021-10-06 11:56:53,588 INFO     #entity: 14951\n",
            "2021-10-06 11:56:53,588 INFO     #relation: 1345\n",
            "2021-10-06 11:56:54,742 INFO     #train: 483142\n",
            "2021-10-06 11:56:54,870 INFO     #valid: 50000\n",
            "2021-10-06 11:56:55,099 INFO     #test: 59071\n",
            "2021-10-06 11:56:55,939 INFO     Model Parameter Configuration:\n",
            "2021-10-06 11:56:55,940 INFO     Parameter gamma: torch.Size([1]), require_grad = False\n",
            "2021-10-06 11:56:55,940 INFO     Parameter embedding_range: torch.Size([1]), require_grad = False\n",
            "2021-10-06 11:56:55,940 INFO     Parameter entity_embedding: torch.Size([14951, 2000]), require_grad = True\n",
            "2021-10-06 11:56:55,940 INFO     Parameter relation_embedding: torch.Size([1345, 1000]), require_grad = True\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "2021-10-06 11:57:03,889 INFO     Ramdomly Initializing RotatE Model...\n",
            "2021-10-06 11:57:03,889 INFO     Start Training...\n",
            "2021-10-06 11:57:03,889 INFO     init_step = 0\n",
            "2021-10-06 11:57:03,889 INFO     batch_size = 1024\n",
            "2021-10-06 11:57:03,889 INFO     negative_adversarial_sampling = 1\n",
            "2021-10-06 11:57:03,889 INFO     hidden_dim = 1000\n",
            "2021-10-06 11:57:03,890 INFO     gamma = 24.000000\n",
            "2021-10-06 11:57:03,890 INFO     negative_adversarial_sampling = True\n",
            "2021-10-06 11:57:03,890 INFO     adversarial_temperature = 1.000000\n",
            "2021-10-06 11:57:03,890 INFO     learning_rate = 0\n",
            "2021-10-06 11:57:07,634 INFO     Training average positive_sample_loss at step 0: 3.169129\n",
            "2021-10-06 11:57:07,635 INFO     Training average negative_sample_loss at step 0: 0.052848\n",
            "2021-10-06 11:57:07,635 INFO     Training average loss at step 0: 1.610988\n",
            "2021-10-06 11:57:07,635 INFO     Evaluating on Valid Dataset...\n",
            "2021-10-06 11:57:09,362 INFO     Evaluating the model... (0/6250)\n",
            "2021-10-06 12:00:03,076 INFO     Evaluating the model... (1000/6250)\n",
            "2021-10-06 12:02:57,637 INFO     Evaluating the model... (2000/6250)\n",
            "2021-10-06 12:05:52,219 INFO     Evaluating the model... (3000/6250)\n",
            "2021-10-06 12:08:45,892 INFO     Evaluating the model... (4000/6250)\n",
            "2021-10-06 12:11:38,436 INFO     Evaluating the model... (5000/6250)\n",
            "2021-10-06 12:14:31,525 INFO     Evaluating the model... (6000/6250)\n",
            "2021-10-06 12:15:14,781 INFO     Valid MRR at step 0: 0.004826\n",
            "2021-10-06 12:15:14,782 INFO     Valid MR at step 0: 7037.281190\n",
            "2021-10-06 12:15:14,782 INFO     Valid HITS@1 at step 0: 0.003960\n",
            "2021-10-06 12:15:14,782 INFO     Valid HITS@3 at step 0: 0.004390\n",
            "2021-10-06 12:15:14,782 INFO     Valid HITS@10 at step 0: 0.005170\n",
            "2021-10-06 12:16:04,237 INFO     Training average positive_sample_loss at step 100: 2.327750\n",
            "2021-10-06 12:16:04,238 INFO     Training average negative_sample_loss at step 100: 0.248345\n",
            "2021-10-06 12:16:04,238 INFO     Training average loss at step 100: 1.288047\n",
            "2021-10-06 12:16:52,316 INFO     Training average positive_sample_loss at step 200: 1.070108\n",
            "2021-10-06 12:16:52,316 INFO     Training average negative_sample_loss at step 200: 0.585440\n",
            "2021-10-06 12:16:52,316 INFO     Training average loss at step 200: 0.827774\n",
            "2021-10-06 12:17:40,466 INFO     Training average positive_sample_loss at step 300: 0.811117\n",
            "2021-10-06 12:17:40,467 INFO     Training average negative_sample_loss at step 300: 0.649537\n",
            "2021-10-06 12:17:40,467 INFO     Training average loss at step 300: 0.730327\n",
            "2021-10-06 12:18:28,690 INFO     Training average positive_sample_loss at step 400: 0.725064\n",
            "2021-10-06 12:18:28,690 INFO     Training average negative_sample_loss at step 400: 0.655478\n",
            "2021-10-06 12:18:28,690 INFO     Training average loss at step 400: 0.690271\n",
            "2021-10-06 12:19:16,755 INFO     Training average positive_sample_loss at step 500: 0.679369\n",
            "2021-10-06 12:19:16,755 INFO     Training average negative_sample_loss at step 500: 0.645542\n",
            "2021-10-06 12:19:16,756 INFO     Training average loss at step 500: 0.662456\n",
            "2021-10-06 12:20:04,988 INFO     Training average positive_sample_loss at step 600: 0.646321\n",
            "2021-10-06 12:20:04,988 INFO     Training average negative_sample_loss at step 600: 0.629776\n",
            "2021-10-06 12:20:04,988 INFO     Training average loss at step 600: 0.638049\n",
            "2021-10-06 12:20:53,347 INFO     Training average positive_sample_loss at step 700: 0.617215\n",
            "2021-10-06 12:20:53,347 INFO     Training average negative_sample_loss at step 700: 0.609387\n",
            "2021-10-06 12:20:53,347 INFO     Training average loss at step 700: 0.613301\n",
            "2021-10-06 12:21:41,676 INFO     Training average positive_sample_loss at step 800: 0.589932\n",
            "2021-10-06 12:21:41,676 INFO     Training average negative_sample_loss at step 800: 0.585988\n",
            "2021-10-06 12:21:41,676 INFO     Training average loss at step 800: 0.587960\n",
            "2021-10-06 12:22:30,118 INFO     Training average positive_sample_loss at step 900: 0.562406\n",
            "2021-10-06 12:22:30,119 INFO     Training average negative_sample_loss at step 900: 0.561877\n",
            "2021-10-06 12:22:30,119 INFO     Training average loss at step 900: 0.562141\n",
            "2021-10-06 12:23:21,187 INFO     Change learning_rate to 0.000010 at step 1000\n",
            "2021-10-06 12:23:21,188 INFO     Training average positive_sample_loss at step 1000: 0.481435\n",
            "2021-10-06 12:23:21,188 INFO     Training average negative_sample_loss at step 1000: 0.515295\n",
            "2021-10-06 12:23:21,188 INFO     Training average loss at step 1000: 0.498365\n",
            "2021-10-06 12:24:09,261 INFO     Training average positive_sample_loss at step 1100: 0.488961\n",
            "2021-10-06 12:24:09,261 INFO     Training average negative_sample_loss at step 1100: 0.435710\n",
            "2021-10-06 12:24:09,262 INFO     Training average loss at step 1100: 0.462336\n",
            "2021-10-06 12:24:57,511 INFO     Training average positive_sample_loss at step 1200: 0.487331\n",
            "2021-10-06 12:24:57,511 INFO     Training average negative_sample_loss at step 1200: 0.434570\n",
            "2021-10-06 12:24:57,511 INFO     Training average loss at step 1200: 0.460950\n",
            "2021-10-06 12:25:45,529 INFO     Training average positive_sample_loss at step 1300: 0.476909\n",
            "2021-10-06 12:25:45,529 INFO     Training average negative_sample_loss at step 1300: 0.437182\n",
            "2021-10-06 12:25:45,529 INFO     Training average loss at step 1300: 0.457046\n",
            "2021-10-06 12:26:33,517 INFO     Training average positive_sample_loss at step 1400: 0.470397\n",
            "2021-10-06 12:26:33,517 INFO     Training average negative_sample_loss at step 1400: 0.438720\n",
            "2021-10-06 12:26:33,517 INFO     Training average loss at step 1400: 0.454558\n",
            "2021-10-06 12:27:21,533 INFO     Training average positive_sample_loss at step 1500: 0.464072\n",
            "2021-10-06 12:27:21,534 INFO     Training average negative_sample_loss at step 1500: 0.438203\n",
            "2021-10-06 12:27:21,534 INFO     Training average loss at step 1500: 0.451138\n",
            "2021-10-06 12:28:09,555 INFO     Training average positive_sample_loss at step 1600: 0.458861\n",
            "2021-10-06 12:28:09,555 INFO     Training average negative_sample_loss at step 1600: 0.437593\n",
            "2021-10-06 12:28:09,555 INFO     Training average loss at step 1600: 0.448227\n",
            "2021-10-06 12:28:57,488 INFO     Training average positive_sample_loss at step 1700: 0.453639\n",
            "2021-10-06 12:28:57,488 INFO     Training average negative_sample_loss at step 1700: 0.437024\n",
            "2021-10-06 12:28:57,488 INFO     Training average loss at step 1700: 0.445332\n",
            "2021-10-06 12:29:45,450 INFO     Training average positive_sample_loss at step 1800: 0.448775\n",
            "2021-10-06 12:29:45,451 INFO     Training average negative_sample_loss at step 1800: 0.434541\n",
            "2021-10-06 12:29:45,451 INFO     Training average loss at step 1800: 0.441658\n",
            "2021-10-06 12:30:36,194 INFO     Training average positive_sample_loss at step 1900: 0.443195\n",
            "2021-10-06 12:30:36,194 INFO     Training average negative_sample_loss at step 1900: 0.434026\n",
            "2021-10-06 12:30:36,194 INFO     Training average loss at step 1900: 0.438611\n",
            "2021-10-06 12:31:25,435 INFO     Evaluating on Valid Dataset...\n",
            "2021-10-06 12:31:27,582 INFO     Evaluating the model... (0/6250)\n",
            "2021-10-06 12:34:19,880 INFO     Evaluating the model... (1000/6250)\n",
            "2021-10-06 12:37:13,408 INFO     Evaluating the model... (2000/6250)\n",
            "2021-10-06 12:40:07,257 INFO     Evaluating the model... (3000/6250)\n",
            "2021-10-06 12:43:00,496 INFO     Evaluating the model... (4000/6250)\n",
            "2021-10-06 12:45:53,500 INFO     Evaluating the model... (5000/6250)\n",
            "2021-10-06 12:48:46,245 INFO     Evaluating the model... (6000/6250)\n",
            "2021-10-06 12:49:29,596 INFO     Valid MRR at step 1999: 0.052081\n",
            "2021-10-06 12:49:29,596 INFO     Valid MR at step 1999: 2059.135000\n",
            "2021-10-06 12:49:29,596 INFO     Valid HITS@1 at step 1999: 0.027680\n",
            "2021-10-06 12:49:29,597 INFO     Valid HITS@3 at step 1999: 0.052330\n",
            "2021-10-06 12:49:29,597 INFO     Valid HITS@10 at step 1999: 0.094400\n",
            "2021-10-06 12:49:29,597 INFO     Evaluating on Test Dataset...\n",
            "2021-10-06 12:49:31,149 INFO     Evaluating the model... (0/7384)\n",
            "2021-10-06 12:52:23,813 INFO     Evaluating the model... (1000/7384)\n",
            "2021-10-06 12:55:17,598 INFO     Evaluating the model... (2000/7384)\n",
            "2021-10-06 12:58:11,156 INFO     Evaluating the model... (3000/7384)\n",
            "2021-10-06 13:01:05,817 INFO     Evaluating the model... (4000/7384)\n",
            "2021-10-06 13:03:59,075 INFO     Evaluating the model... (5000/7384)\n",
            "2021-10-06 13:06:53,267 INFO     Evaluating the model... (6000/7384)\n",
            "2021-10-06 13:09:47,177 INFO     Evaluating the model... (7000/7384)\n",
            "2021-10-06 13:10:53,949 INFO     Test MRR at step 1999: 0.051508\n",
            "2021-10-06 13:10:53,949 INFO     Test MR at step 1999: 2081.780908\n",
            "2021-10-06 13:10:53,949 INFO     Test HITS@1 at step 1999: 0.026951\n",
            "2021-10-06 13:10:53,949 INFO     Test HITS@3 at step 1999: 0.052022\n",
            "2021-10-06 13:10:53,950 INFO     Test HITS@10 at step 1999: 0.094733\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mC7JrLHNSpUm"
      },
      "source": [
        "## OpenBioLink with RotE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7FObgrWLrCZ"
      },
      "source": [
        "#!/usr/bin/python3\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import logging\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from sklearn.metrics import average_precision_score\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from dataloader import TestDataset\n",
        "\n",
        "class KGEModel(nn.Module):\n",
        "    def __init__(self, model_name, nentity, nrelation, hidden_dim, gamma, \n",
        "                 double_entity_embedding=False, double_relation_embedding=False):\n",
        "        super(KGEModel, self).__init__()\n",
        "        self.model_name = model_name\n",
        "        self.nentity = nentity\n",
        "        self.nrelation = nrelation\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.epsilon = 2.0\n",
        "        \n",
        "        self.gamma = nn.Parameter(\n",
        "            torch.Tensor([gamma]), \n",
        "            requires_grad=False\n",
        "        )\n",
        "        \n",
        "        self.embedding_range = nn.Parameter(\n",
        "            torch.Tensor([(self.gamma.item() + self.epsilon) / hidden_dim]), \n",
        "            requires_grad=False\n",
        "        )\n",
        "        \n",
        "        self.entity_dim = hidden_dim*2 if double_entity_embedding else hidden_dim\n",
        "        self.relation_dim = hidden_dim*2 if double_relation_embedding else hidden_dim\n",
        "        \n",
        "        self.entity_embedding = nn.Parameter(torch.zeros(nentity, self.entity_dim))\n",
        "        nn.init.uniform_(\n",
        "            tensor=self.entity_embedding, \n",
        "            a=-self.embedding_range.item(), \n",
        "            b=self.embedding_range.item()\n",
        "        )\n",
        "        \n",
        "        self.relation_embedding = nn.Parameter(torch.zeros(nrelation, self.relation_dim))\n",
        "        nn.init.uniform_(\n",
        "            tensor=self.relation_embedding, \n",
        "            a=-self.embedding_range.item(), \n",
        "            b=self.embedding_range.item()\n",
        "        )\n",
        "        \n",
        "        if model_name == 'pRotatE':\n",
        "            self.modulus = nn.Parameter(torch.Tensor([[0.5 * self.embedding_range.item()]]))\n",
        "        \n",
        "        #Do not forget to modify this line when you add a new model in the \"forward\" function\n",
        "        if model_name not in ['TransE', 'DistMult', 'ComplEx', 'RotatE', 'pRotatE']:\n",
        "            raise ValueError('model %s not supported' % model_name)\n",
        "            \n",
        "        if model_name == 'RotatE' and (not double_entity_embedding or double_relation_embedding):\n",
        "            raise ValueError('RotatE should use --double_entity_embedding')\n",
        "\n",
        "        if model_name == 'ComplEx' and (not double_entity_embedding or not double_relation_embedding):\n",
        "            raise ValueError('ComplEx should use --double_entity_embedding and --double_relation_embedding')\n",
        "        \n",
        "    def forward(self, sample, mode='single'):\n",
        "        '''\n",
        "        Forward function that calculate the score of a batch of triples.\n",
        "        In the 'single' mode, sample is a batch of triple.\n",
        "        In the 'head-batch' or 'tail-batch' mode, sample consists two part.\n",
        "        The first part is usually the positive sample.\n",
        "        And the second part is the entities in the negative samples.\n",
        "        Because negative samples and positive samples usually share two elements \n",
        "        in their triple ((head, relation) or (relation, tail)).\n",
        "        '''\n",
        "\n",
        "        if mode == 'single':\n",
        "            batch_size, negative_sample_size = sample.size(0), 1\n",
        "            \n",
        "            head = torch.index_select(\n",
        "                self.entity_embedding, \n",
        "                dim=0, \n",
        "                index=sample[:,0]\n",
        "            ).unsqueeze(1)\n",
        "            \n",
        "            relation = torch.index_select(\n",
        "                self.relation_embedding, \n",
        "                dim=0, \n",
        "                index=sample[:,1]\n",
        "            ).unsqueeze(1)\n",
        "            \n",
        "            tail = torch.index_select(\n",
        "                self.entity_embedding, \n",
        "                dim=0, \n",
        "                index=sample[:,2]\n",
        "            ).unsqueeze(1)\n",
        "            \n",
        "        elif mode == 'head-batch':\n",
        "            tail_part, head_part = sample\n",
        "            batch_size, negative_sample_size = head_part.size(0), head_part.size(1)\n",
        "            \n",
        "            head = torch.index_select(\n",
        "                self.entity_embedding, \n",
        "                dim=0, \n",
        "                index=head_part.view(-1)\n",
        "            ).view(batch_size, negative_sample_size, -1)\n",
        "            \n",
        "            relation = torch.index_select(\n",
        "                self.relation_embedding, \n",
        "                dim=0, \n",
        "                index=tail_part[:, 1]\n",
        "            ).unsqueeze(1)\n",
        "            \n",
        "            tail = torch.index_select(\n",
        "                self.entity_embedding, \n",
        "                dim=0, \n",
        "                index=tail_part[:, 2]\n",
        "            ).unsqueeze(1)\n",
        "            \n",
        "        elif mode == 'tail-batch':\n",
        "            head_part, tail_part = sample\n",
        "            batch_size, negative_sample_size = tail_part.size(0), tail_part.size(1)\n",
        "            \n",
        "            head = torch.index_select(\n",
        "                self.entity_embedding, \n",
        "                dim=0, \n",
        "                index=head_part[:, 0]\n",
        "            ).unsqueeze(1)\n",
        "            \n",
        "            relation = torch.index_select(\n",
        "                self.relation_embedding,\n",
        "                dim=0,\n",
        "                index=head_part[:, 1]\n",
        "            ).unsqueeze(1)\n",
        "            \n",
        "            tail = torch.index_select(\n",
        "                self.entity_embedding, \n",
        "                dim=0, \n",
        "                index=tail_part.view(-1)\n",
        "            ).view(batch_size, negative_sample_size, -1)\n",
        "            \n",
        "        else:\n",
        "            raise ValueError('mode %s not supported' % mode)\n",
        "            \n",
        "        model_func = {\n",
        "            'TransE': self.TransE,\n",
        "            'DistMult': self.DistMult,\n",
        "            'ComplEx': self.ComplEx,\n",
        "            'RotatE': self.RotatE,\n",
        "            'pRotatE': self.pRotatE\n",
        "        }\n",
        "        \n",
        "        if self.model_name in model_func:\n",
        "            score = model_func[self.model_name](head, relation, tail, mode)\n",
        "        else:\n",
        "            raise ValueError('model %s not supported' % self.model_name)\n",
        "        \n",
        "        return score\n",
        "    \n",
        "    def TransE(self, head, relation, tail, mode):\n",
        "        if mode == 'head-batch':\n",
        "            score = head + (relation - tail)\n",
        "        else:\n",
        "            score = (head + relation) - tail\n",
        "\n",
        "        score = self.gamma.item() - torch.norm(score, p=1, dim=2)\n",
        "        return score\n",
        "\n",
        "    def DistMult(self, head, relation, tail, mode):\n",
        "        if mode == 'head-batch':\n",
        "            score = head * (relation * tail)\n",
        "        else:\n",
        "            score = (head * relation) * tail\n",
        "\n",
        "        score = score.sum(dim = 2)\n",
        "        return score\n",
        "\n",
        "    def ComplEx(self, head, relation, tail, mode):\n",
        "        re_head, im_head = torch.chunk(head, 2, dim=2)\n",
        "        re_relation, im_relation = torch.chunk(relation, 2, dim=2)\n",
        "        re_tail, im_tail = torch.chunk(tail, 2, dim=2)\n",
        "\n",
        "        if mode == 'head-batch':\n",
        "            re_score = re_relation * re_tail + im_relation * im_tail\n",
        "            im_score = re_relation * im_tail - im_relation * re_tail\n",
        "            score = re_head * re_score + im_head * im_score\n",
        "        else:\n",
        "            re_score = re_head * re_relation - im_head * im_relation\n",
        "            im_score = re_head * im_relation + im_head * re_relation\n",
        "            score = re_score * re_tail + im_score * im_tail\n",
        "\n",
        "        score = score.sum(dim = 2)\n",
        "        return score\n",
        "\n",
        "    def RotatE(self, head, relation, tail, mode):\n",
        "        pi = 3.14159265358979323846\n",
        "        \n",
        "        re_head, im_head = torch.chunk(head, 2, dim=2)\n",
        "        re_tail, im_tail = torch.chunk(tail, 2, dim=2)\n",
        "\n",
        "        #Make phases of relations uniformly distributed in [-pi, pi]\n",
        "\n",
        "        phase_relation = relation/(self.embedding_range.item()/pi)\n",
        "\n",
        "        re_relation = torch.cos(phase_relation)\n",
        "        im_relation = torch.sin(phase_relation)\n",
        "\n",
        "        if mode == 'head-batch':\n",
        "            re_score = re_relation * re_tail + im_relation * im_tail\n",
        "            im_score = re_relation * im_tail - im_relation * re_tail\n",
        "            re_score = re_score - re_head\n",
        "            im_score = im_score - im_head\n",
        "        else:\n",
        "            re_score = re_head * re_relation - im_head * im_relation\n",
        "            im_score = re_head * im_relation + im_head * re_relation\n",
        "            re_score = re_score - re_tail\n",
        "            im_score = im_score - im_tail\n",
        "\n",
        "        score = torch.stack([re_score, im_score], dim = 0)\n",
        "        score = score.norm(dim = 0)\n",
        "\n",
        "        score = self.gamma.item() - score.sum(dim = 2)\n",
        "        return score\n",
        "\n",
        "    def pRotatE(self, head, relation, tail, mode):\n",
        "        pi = 3.14159262358979323846\n",
        "        \n",
        "        #Make phases of entities and relations uniformly distributed in [-pi, pi]\n",
        "\n",
        "        phase_head = head/(self.embedding_range.item()/pi)\n",
        "        phase_relation = relation/(self.embedding_range.item()/pi)\n",
        "        phase_tail = tail/(self.embedding_range.item()/pi)\n",
        "\n",
        "        if mode == 'head-batch':\n",
        "            score = phase_head + (phase_relation - phase_tail)\n",
        "        else:\n",
        "            score = (phase_head + phase_relation) - phase_tail\n",
        "\n",
        "        score = torch.sin(score)            \n",
        "        score = torch.abs(score)\n",
        "\n",
        "        score = self.gamma.item() - score.sum(dim = 2) * self.modulus\n",
        "        return score\n",
        "    \n",
        "    @staticmethod\n",
        "    def train_step(model, optimizer, train_iterator, args):\n",
        "        '''\n",
        "        A single train step. Apply back-propation and return the loss\n",
        "        '''\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        positive_sample, negative_sample, subsampling_weight, mode = next(train_iterator)\n",
        "\n",
        "        if args.cuda:\n",
        "            positive_sample = positive_sample.cuda()\n",
        "            negative_sample = negative_sample.cuda()\n",
        "            subsampling_weight = subsampling_weight.cuda()\n",
        "\n",
        "        negative_score = model((positive_sample, negative_sample), mode=mode)\n",
        "\n",
        "        if args.negative_adversarial_sampling:\n",
        "            #In self-adversarial sampling, we do not apply back-propagation on the sampling weight\n",
        "            negative_score = (F.softmax(negative_score * args.adversarial_temperature, dim = 1).detach() \n",
        "                              * F.logsigmoid(-negative_score)).sum(dim = 1)\n",
        "        else:\n",
        "            negative_score = F.logsigmoid(-negative_score).mean(dim = 1)\n",
        "\n",
        "        positive_score = model(positive_sample)\n",
        "\n",
        "        positive_score = F.logsigmoid(positive_score).squeeze(dim = 1)\n",
        "\n",
        "        if args.uni_weight:\n",
        "            positive_sample_loss = - positive_score.mean()\n",
        "            negative_sample_loss = - negative_score.mean()\n",
        "        else:\n",
        "            positive_sample_loss = - (subsampling_weight * positive_score).sum()/subsampling_weight.sum()\n",
        "            negative_sample_loss = - (subsampling_weight * negative_score).sum()/subsampling_weight.sum()\n",
        "\n",
        "        loss = (positive_sample_loss + negative_sample_loss)/2\n",
        "        \n",
        "        if args.regularization != 0.0:\n",
        "            #Use L3 regularization for ComplEx and DistMult\n",
        "            regularization = args.regularization * (\n",
        "                model.entity_embedding.norm(p = 3)**3 + \n",
        "                model.relation_embedding.norm(p = 3).norm(p = 3)**3\n",
        "            )\n",
        "            loss = loss + regularization\n",
        "            regularization_log = {'regularization': regularization.item()}\n",
        "        else:\n",
        "            regularization_log = {}\n",
        "            \n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        log = {\n",
        "            **regularization_log,\n",
        "            'positive_sample_loss': positive_sample_loss.item(),\n",
        "            'negative_sample_loss': negative_sample_loss.item(),\n",
        "            'loss': loss.item()\n",
        "        }\n",
        "\n",
        "        return log\n",
        "    \n",
        "    @staticmethod\n",
        "    def test_step(model, test_triples, all_true_triples, args):\n",
        "        '''\n",
        "        Evaluate the model on test or valid datasets\n",
        "        '''\n",
        "        \n",
        "        model.eval()\n",
        "        \n",
        "        if args.countries:\n",
        "            #Countries S* datasets are evaluated on AUC-PR\n",
        "            #Process test data for AUC-PR evaluation\n",
        "            sample = list()\n",
        "            y_true  = list()\n",
        "            for head, relation, tail in test_triples:\n",
        "                for candidate_region in args.regions:\n",
        "                    y_true.append(1 if candidate_region == tail else 0)\n",
        "                    sample.append((head, relation, candidate_region))\n",
        "\n",
        "            sample = torch.LongTensor(sample)\n",
        "            if args.cuda:\n",
        "                sample = sample.cuda()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                y_score = model(sample).squeeze(1).cpu().numpy()\n",
        "\n",
        "            y_true = np.array(y_true)\n",
        "\n",
        "            #average_precision_score is the same as auc_pr\n",
        "            auc_pr = average_precision_score(y_true, y_score)\n",
        "\n",
        "            metrics = {'auc_pr': auc_pr}\n",
        "            \n",
        "        else:\n",
        "            #Otherwise use standard (filtered) MRR, MR, HITS@1, HITS@3, and HITS@10 metrics\n",
        "            #Prepare dataloader for evaluation\n",
        "            test_dataloader_head = DataLoader(\n",
        "                TestDataset(\n",
        "                    test_triples, \n",
        "                    all_true_triples, \n",
        "                    args.nentity, \n",
        "                    args.nrelation, \n",
        "                    'head-batch'\n",
        "                ), \n",
        "                batch_size=args.test_batch_size,\n",
        "                num_workers=max(1, args.cpu_num//2), \n",
        "                collate_fn=TestDataset.collate_fn\n",
        "            )\n",
        "\n",
        "            test_dataloader_tail = DataLoader(\n",
        "                TestDataset(\n",
        "                    test_triples, \n",
        "                    all_true_triples, \n",
        "                    args.nentity, \n",
        "                    args.nrelation, \n",
        "                    'tail-batch'\n",
        "                ), \n",
        "                batch_size=args.test_batch_size,\n",
        "                num_workers=max(1, args.cpu_num//2), \n",
        "                collate_fn=TestDataset.collate_fn\n",
        "            )\n",
        "            \n",
        "            test_dataset_list = [test_dataloader_head, test_dataloader_tail]\n",
        "            \n",
        "            logs = []\n",
        "\n",
        "            step = 0\n",
        "            total_steps = sum([len(dataset) for dataset in test_dataset_list])\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for test_dataset in test_dataset_list:\n",
        "                    for positive_sample, negative_sample, filter_bias, mode in test_dataset:\n",
        "                        if args.cuda:\n",
        "                            positive_sample = positive_sample.cuda()\n",
        "                            negative_sample = negative_sample.cuda()\n",
        "                            filter_bias = filter_bias.cuda()\n",
        "\n",
        "                        batch_size = positive_sample.size(0)\n",
        "\n",
        "                        score = model((positive_sample, negative_sample), mode)\n",
        "                        score += filter_bias\n",
        "\n",
        "                        #Explicitly sort all the entities to ensure that there is no test exposure bias\n",
        "                        argsort = torch.argsort(score, dim = 1, descending=True)\n",
        "\n",
        "                        if mode == 'head-batch':\n",
        "                            positive_arg = positive_sample[:, 0]\n",
        "                        elif mode == 'tail-batch':\n",
        "                            positive_arg = positive_sample[:, 2]\n",
        "                        else:\n",
        "                            raise ValueError('mode %s not supported' % mode)\n",
        "\n",
        "                        for i in range(batch_size):\n",
        "                            #Notice that argsort is not ranking\n",
        "                            ranking = (argsort[i, :] == positive_arg[i]).nonzero()\n",
        "                            assert ranking.size(0) == 1\n",
        "\n",
        "                            #ranking + 1 is the true ranking used in evaluation metrics\n",
        "                            ranking = 1 + ranking.item()\n",
        "                            logs.append({\n",
        "                                'MRR': 1.0/ranking,\n",
        "                                'MR': float(ranking),\n",
        "                                'HITS@1': 1.0 if ranking <= 1 else 0.0,\n",
        "                                'HITS@3': 1.0 if ranking <= 3 else 0.0,\n",
        "                                'HITS@10': 1.0 if ranking <= 10 else 0.0,\n",
        "                            })\n",
        "\n",
        "                        if step % args.test_log_steps == 0:\n",
        "                            logging.info('Evaluating the model... (%d/%d)' % (step, total_steps))\n",
        "\n",
        "                        step += 1\n",
        "\n",
        "            metrics = {}\n",
        "            for metric in logs[0].keys():\n",
        "                metrics[metric] = sum([log[metric] for log in logs])/len(logs)\n",
        "\n",
        "        return metrics\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXmLSYhCtrdX"
      },
      "source": [
        "#!/usr/bin/python3\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import argparse\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from model import KGEModel\n",
        "\n",
        "from dataloader import TrainDataset\n",
        "from dataloader import BidirectionalOneShotIterator\n",
        "\n",
        "def parse_args(args=None):\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description='Training and Testing Knowledge Graph Embedding Models',\n",
        "        usage='train.py [<args>] [-h | --help]'\n",
        "    )\n",
        "\n",
        "    parser.add_argument('--cuda', action='store_true', help='use GPU')\n",
        "    \n",
        "    parser.add_argument('--do_train', action='store_true')\n",
        "    parser.add_argument('--do_valid', action='store_true')\n",
        "    parser.add_argument('--do_test', action='store_true')\n",
        "    parser.add_argument('--evaluate_train', action='store_true', help='Evaluate on training data')\n",
        "    \n",
        "\n",
        "    \n",
        "    parser.add_argument('--data_path', type=str, default=None)\n",
        "    parser.add_argument('--model', default='TransE', type=str)\n",
        "    parser.add_argument('-de', '--double_entity_embedding', action='store_true')\n",
        "    parser.add_argument('-dr', '--double_relation_embedding', action='store_true')\n",
        "    \n",
        "    parser.add_argument('-n', '--negative_sample_size', default=128, type=int)\n",
        "    parser.add_argument('-d', '--hidden_dim', default=500, type=int)\n",
        "    parser.add_argument('-g', '--gamma', default=12.0, type=float)\n",
        "    parser.add_argument('-adv', '--negative_adversarial_sampling', action='store_true')\n",
        "    parser.add_argument('-a', '--adversarial_temperature', default=1.0, type=float)\n",
        "    parser.add_argument('-b', '--batch_size', default=1024, type=int)\n",
        "    parser.add_argument('-r', '--regularization', default=0.0, type=float)\n",
        "    parser.add_argument('--test_batch_size', default=4, type=int, help='valid/test batch size')\n",
        "    parser.add_argument('--uni_weight', action='store_true', \n",
        "                        help='Otherwise use subsampling weighting like in word2vec')\n",
        "    \n",
        "    parser.add_argument('-lr', '--learning_rate', default=0.0001, type=float)\n",
        "    parser.add_argument('-cpu', '--cpu_num', default=10, type=int)\n",
        "    parser.add_argument('-init', '--init_checkpoint', default=None, type=str)\n",
        "    parser.add_argument('-save', '--save_path', default=None, type=str)\n",
        "    parser.add_argument('--max_steps', default=100000, type=int)\n",
        "    parser.add_argument('--warm_up_steps', default=None, type=int)\n",
        "    \n",
        "    parser.add_argument('--save_checkpoint_steps', default=10000, type=int)\n",
        "    parser.add_argument('--valid_steps', default=10000, type=int)\n",
        "    parser.add_argument('--log_steps', default=100, type=int, help='train log every xx steps')\n",
        "    parser.add_argument('--test_log_steps', default=1000, type=int, help='valid/test log every xx steps')\n",
        "    \n",
        "    parser.add_argument('--nentity', type=int, default=0, help='DO NOT MANUALLY SET')\n",
        "    parser.add_argument('--nrelation', type=int, default=0, help='DO NOT MANUALLY SET')\n",
        "    \n",
        "    return parser.parse_args(args)\n",
        "\n",
        "def override_config(args):\n",
        "    '''\n",
        "    Override model and data configuration\n",
        "    '''\n",
        "    \n",
        "    with open(os.path.join(args.init_checkpoint, 'config.json'), 'r') as fjson:\n",
        "        argparse_dict = json.load(fjson)\n",
        "    \n",
        "    args.countries = argparse_dict['countries']\n",
        "    if args.data_path is None:\n",
        "        args.data_path = argparse_dict['data_path']\n",
        "    args.model = argparse_dict['model']\n",
        "    args.double_entity_embedding = argparse_dict['double_entity_embedding']\n",
        "    args.double_relation_embedding = argparse_dict['double_relation_embedding']\n",
        "    args.hidden_dim = argparse_dict['hidden_dim']\n",
        "    args.test_batch_size = argparse_dict['test_batch_size']\n",
        "    \n",
        "def save_model(model, optimizer, save_variable_list, args):\n",
        "    '''\n",
        "    Save the parameters of the model and the optimizer,\n",
        "    as well as some other variables such as step and learning_rate\n",
        "    '''\n",
        "    \n",
        "    argparse_dict = vars(args)\n",
        "    with open(os.path.join(args.save_path, 'config.json'), 'w') as fjson:\n",
        "        json.dump(argparse_dict, fjson)\n",
        "\n",
        "    torch.save({\n",
        "        **save_variable_list,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict()},\n",
        "        os.path.join(args.save_path, 'checkpoint')\n",
        "    )\n",
        "    \n",
        "    entity_embedding = model.entity_embedding.detach().cpu().numpy()\n",
        "    np.save(\n",
        "        os.path.join(args.save_path, 'entity_embedding'), \n",
        "        entity_embedding\n",
        "    )\n",
        "    \n",
        "    relation_embedding = model.relation_embedding.detach().cpu().numpy()\n",
        "    np.save(\n",
        "        os.path.join(args.save_path, 'relation_embedding'), \n",
        "        relation_embedding\n",
        "    )\n",
        "\n",
        "def read_triple(file_path, entity2id, relation2id):\n",
        "    '''\n",
        "    Read triples and map them into ids.\n",
        "    '''\n",
        "    triples = []\n",
        "    with open(file_path) as fin:\n",
        "        for line in fin:\n",
        "            h, r, t = line.strip().split('\\t')\n",
        "            triples.append((entity2id[h], relation2id[r], entity2id[t]))\n",
        "    return triples\n",
        "\n",
        "def set_logger(args):\n",
        "    '''\n",
        "    Write logs to checkpoint and console\n",
        "    '''\n",
        "\n",
        "    if args.do_train:\n",
        "        log_file = os.path.join(args.save_path or args.init_checkpoint, 'train.log')\n",
        "    else:\n",
        "        log_file = os.path.join(args.save_path or args.init_checkpoint, 'test.log')\n",
        "\n",
        "    logging.basicConfig(\n",
        "        format='%(asctime)s %(levelname)-8s %(message)s',\n",
        "        level=logging.INFO,\n",
        "        datefmt='%Y-%m-%d %H:%M:%S',\n",
        "        filename=log_file,\n",
        "        filemode='w'\n",
        "    )\n",
        "    console = logging.StreamHandler()\n",
        "    console.setLevel(logging.INFO)\n",
        "    formatter = logging.Formatter('%(asctime)s %(levelname)-8s %(message)s')\n",
        "    console.setFormatter(formatter)\n",
        "    logging.getLogger('').addHandler(console)\n",
        "\n",
        "def log_metrics(mode, step, metrics):\n",
        "    '''\n",
        "    Print the evaluation logs\n",
        "    '''\n",
        "    for metric in metrics:\n",
        "        logging.info('%s %s at step %d: %f' % (mode, metric, step, metrics[metric]))\n",
        "        \n",
        "        \n",
        "def main(args):\n",
        "    if (not args.do_train) and (not args.do_valid) and (not args.do_test):\n",
        "        raise ValueError('one of train/val/test mode must be choosed.')\n",
        "    \n",
        "    if args.init_checkpoint:\n",
        "        override_config(args)\n",
        "    elif args.data_path is None:\n",
        "        raise ValueError('one of init_checkpoint/data_path must be choosed.')\n",
        "\n",
        "    if args.do_train and args.save_path is None:\n",
        "        raise ValueError('Where do you want to save your trained model?')\n",
        "    \n",
        "    if args.save_path and not os.path.exists(args.save_path):\n",
        "        os.makedirs(args.save_path)\n",
        "    \n",
        "    # Write logs to checkpoint and console\n",
        "    set_logger(args)\n",
        "    \n",
        "    with open(os.path.join(args.data_path, 'entities.dict')) as fin:\n",
        "        entity2id = dict()\n",
        "        for line in fin:\n",
        "            eid, entity = line.strip().split('\\t')\n",
        "            entity2id[entity] = int(eid)\n",
        "\n",
        "    with open(os.path.join(args.data_path, 'relations.dict')) as fin:\n",
        "        relation2id = dict()\n",
        "        for line in fin:\n",
        "            rid, relation = line.strip().split('\\t')\n",
        "            relation2id[relation] = int(rid)\n",
        "    \n",
        "    # Read regions for Countries S* datasets\n",
        "    if args.countries:\n",
        "        regions = list()\n",
        "        with open(os.path.join(args.data_path, 'regions.list')) as fin:\n",
        "            for line in fin:\n",
        "                region = line.strip()\n",
        "                regions.append(entity2id[region])\n",
        "        args.regions = regions\n",
        "\n",
        "    nentity = len(entity2id)\n",
        "    nrelation = len(relation2id)\n",
        "    \n",
        "    args.nentity = nentity\n",
        "    args.nrelation = nrelation\n",
        "    \n",
        "    logging.info('Model: %s' % args.model)\n",
        "    logging.info('Data Path: %s' % args.data_path)\n",
        "    logging.info('#entity: %d' % nentity)\n",
        "    logging.info('#relation: %d' % nrelation)\n",
        "    \n",
        "    train_triples = read_triple(os.path.join(args.data_path, 'train.txt'), entity2id, relation2id)\n",
        "    logging.info('#train: %d' % len(train_triples))\n",
        "    valid_triples = read_triple(os.path.join(args.data_path, 'valid.txt'), entity2id, relation2id)\n",
        "    logging.info('#valid: %d' % len(valid_triples))\n",
        "    test_triples = read_triple(os.path.join(args.data_path, 'test.txt'), entity2id, relation2id)\n",
        "    logging.info('#test: %d' % len(test_triples))\n",
        "    \n",
        "    #All true triples\n",
        "    all_true_triples = train_triples + valid_triples + test_triples\n",
        "    \n",
        "    kge_model = KGEModel(\n",
        "        model_name=args.model,\n",
        "        nentity=nentity,\n",
        "        nrelation=nrelation,\n",
        "        hidden_dim=args.hidden_dim,\n",
        "        gamma=args.gamma,\n",
        "        double_entity_embedding=args.double_entity_embedding,\n",
        "        double_relation_embedding=args.double_relation_embedding\n",
        "    )\n",
        "    \n",
        "    logging.info('Model Parameter Configuration:')\n",
        "    for name, param in kge_model.named_parameters():\n",
        "        logging.info('Parameter %s: %s, require_grad = %s' % (name, str(param.size()), str(param.requires_grad)))\n",
        "\n",
        "    if args.cuda:\n",
        "        kge_model = kge_model.cuda()\n",
        "    \n",
        "    if args.do_train:\n",
        "        # Set training dataloader iterator\n",
        "        train_dataloader_head = DataLoader(\n",
        "            TrainDataset(train_triples, nentity, nrelation, args.negative_sample_size, 'head-batch'), \n",
        "            batch_size=args.batch_size,\n",
        "            shuffle=True, \n",
        "            num_workers=max(1, args.cpu_num//2),\n",
        "            collate_fn=TrainDataset.collate_fn\n",
        "        )\n",
        "        \n",
        "        train_dataloader_tail = DataLoader(\n",
        "            TrainDataset(train_triples, nentity, nrelation, args.negative_sample_size, 'tail-batch'), \n",
        "            batch_size=args.batch_size,\n",
        "            shuffle=True, \n",
        "            num_workers=max(1, args.cpu_num//2),\n",
        "            collate_fn=TrainDataset.collate_fn\n",
        "        )\n",
        "        \n",
        "        train_iterator = BidirectionalOneShotIterator(train_dataloader_head, train_dataloader_tail)\n",
        "        \n",
        "        # Set training configuration\n",
        "        current_learning_rate = args.learning_rate\n",
        "        optimizer = torch.optim.Adam(\n",
        "            filter(lambda p: p.requires_grad, kge_model.parameters()), \n",
        "            lr=current_learning_rate\n",
        "        )\n",
        "        if args.warm_up_steps:\n",
        "            warm_up_steps = args.warm_up_steps\n",
        "        else:\n",
        "            warm_up_steps = args.max_steps // 2\n",
        "\n",
        "    if args.init_checkpoint:\n",
        "        # Restore model from checkpoint directory\n",
        "        logging.info('Loading checkpoint %s...' % args.init_checkpoint)\n",
        "        checkpoint = torch.load(os.path.join(args.init_checkpoint, 'checkpoint'))\n",
        "        init_step = checkpoint['step']\n",
        "        kge_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        if args.do_train:\n",
        "            current_learning_rate = checkpoint['current_learning_rate']\n",
        "            warm_up_steps = checkpoint['warm_up_steps']\n",
        "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    else:\n",
        "        logging.info('Ramdomly Initializing %s Model...' % args.model)\n",
        "        init_step = 0\n",
        "    \n",
        "    step = init_step\n",
        "    \n",
        "    logging.info('Start Training...')\n",
        "    logging.info('init_step = %d' % init_step)\n",
        "    logging.info('batch_size = %d' % args.batch_size)\n",
        "    logging.info('negative_adversarial_sampling = %d' % args.negative_adversarial_sampling)\n",
        "    logging.info('hidden_dim = %d' % args.hidden_dim)\n",
        "    logging.info('gamma = %f' % args.gamma)\n",
        "    logging.info('negative_adversarial_sampling = %s' % str(args.negative_adversarial_sampling))\n",
        "    if args.negative_adversarial_sampling:\n",
        "        logging.info('adversarial_temperature = %f' % args.adversarial_temperature)\n",
        "    \n",
        "    # Set valid dataloader as it would be evaluated during training\n",
        "    \n",
        "    if args.do_train:\n",
        "        logging.info('learning_rate = %d' % current_learning_rate)\n",
        "\n",
        "        training_logs = []\n",
        "        \n",
        "        #Training Loop\n",
        "        for step in range(init_step, args.max_steps):\n",
        "            \n",
        "            log = kge_model.train_step(kge_model, optimizer, train_iterator, args)\n",
        "            \n",
        "            training_logs.append(log)\n",
        "            \n",
        "            if step >= warm_up_steps:\n",
        "                current_learning_rate = current_learning_rate / 10\n",
        "                logging.info('Change learning_rate to %f at step %d' % (current_learning_rate, step))\n",
        "                optimizer = torch.optim.Adam(\n",
        "                    filter(lambda p: p.requires_grad, kge_model.parameters()), \n",
        "                    lr=current_learning_rate\n",
        "                )\n",
        "                warm_up_steps = warm_up_steps * 3\n",
        "            \n",
        "            if step % args.save_checkpoint_steps == 0:\n",
        "                save_variable_list = {\n",
        "                    'step': step, \n",
        "                    'current_learning_rate': current_learning_rate,\n",
        "                    'warm_up_steps': warm_up_steps\n",
        "                }\n",
        "                save_model(kge_model, optimizer, save_variable_list, args)\n",
        "                \n",
        "            if step % args.log_steps == 0:\n",
        "                metrics = {}\n",
        "                for metric in training_logs[0].keys():\n",
        "                    metrics[metric] = sum([log[metric] for log in training_logs])/len(training_logs)\n",
        "                log_metrics('Training average', step, metrics)\n",
        "                training_logs = []\n",
        "                \n",
        "            if args.do_valid and step % args.valid_steps == 0:\n",
        "                logging.info('Evaluating on Valid Dataset...')\n",
        "                metrics = kge_model.test_step(kge_model, valid_triples, all_true_triples, args)\n",
        "                log_metrics('Valid', step, metrics)\n",
        "        \n",
        "        save_variable_list = {\n",
        "            'step': step, \n",
        "            'current_learning_rate': current_learning_rate,\n",
        "            'warm_up_steps': warm_up_steps\n",
        "        }\n",
        "        save_model(kge_model, optimizer, save_variable_list, args)\n",
        "        \n",
        "    if args.do_valid:\n",
        "        logging.info('Evaluating on Valid Dataset...')\n",
        "        metrics = kge_model.test_step(kge_model, valid_triples, all_true_triples, args)\n",
        "        log_metrics('Valid', step, metrics)\n",
        "    \n",
        "    if args.do_test:\n",
        "        logging.info('Evaluating on Test Dataset...')\n",
        "        metrics = kge_model.test_step(kge_model, test_triples, all_true_triples, args)\n",
        "        log_metrics('Test', step, metrics)\n",
        "    \n",
        "    if args.evaluate_train:\n",
        "        logging.info('Evaluating on Training Dataset...')\n",
        "        metrics = kge_model.test_step(kge_model, train_triples, all_true_triples, args)\n",
        "        log_metrics('Test', step, metrics)\n",
        "        \n",
        "if __name__ == '__main__':\n",
        "    main(parse_args())\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}